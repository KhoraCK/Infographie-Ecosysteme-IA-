# Analyse d'Ã‰ligibilitÃ© CIR â€” Plateforme IA WIDIP

**Projet** : WIBOT / WIDIP_ARCHI â€” Plateforme IA de support informatique proactif
**Date d'analyse** : 3 fÃ©vrier 2026
**RÃ©fÃ©rentiel** : Manuel de Frascati (OCDE) + Guide CIR 2025 (MESR)

---

## Vue d'ensemble

La plateforme WIDIP est un systÃ¨me multi-agents IA conÃ§u pour le support informatique. Elle combine une interface conversationnelle (WIBOT), trois workflows autonomes (Sentinel, Assistant Ticket, Enrichisseur), un framework de sÃ©curitÃ© (SAFEGUARD) et une base de connaissances Ã©volutive (RAG).

L'analyse ci-dessous identifie **5 composants** porteurs de potentiel R&D, organisÃ©s en **3 opÃ©rations de recherche** structurÃ©es selon les 5 critÃ¨res de Frascati.

---

## Cartographie R&D

```
                    PLATEFORME IA WIDIP
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚  OR 1   â”‚      â”‚  OR 2   â”‚      â”‚  OR 3   â”‚
   â”‚Capitali-â”‚      â”‚Qualifi- â”‚      â”‚Gouver-  â”‚
   â”‚sation   â”‚      â”‚cation   â”‚      â”‚nance    â”‚
   â”‚Auto.    â”‚      â”‚Autonome â”‚      â”‚SÃ©curitÃ© â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚Enrichis-â”‚      â”‚Assistantâ”‚      â”‚SAFEGUARDâ”‚
   â”‚seur     â”‚      â”‚Ticket   â”‚      â”‚Multi-   â”‚
   â”‚+ RAG    â”‚      â”‚+ Conf.  â”‚      â”‚Agents   â”‚
   â”‚Ã©volutif â”‚      â”‚calibrÃ©e â”‚      â”‚         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â–  Potentiel FORT     â–  Potentiel MOYEN
   - Enrichisseur       - Sentinel
   - Assistant Ticket   - RAG Ã©volutif
   - SAFEGUARD
```

---

---

# OPÃ‰RATION R&D 1 â€” Capitalisation automatique des connaissances IT par IA

**Composants concernÃ©s** : Enrichisseur + RAG Ã©volutif
**Potentiel CIR** : ğŸŸ¢ FORT (Enrichisseur) + ğŸŸ¡ MOYEN (RAG)

---

## ProblÃ©matique scientifique

> Comment transformer automatiquement l'historique opÃ©rationnel non structurÃ© d'une PME (tickets, conversations, rÃ©solutions) en une base de connaissances fiable, Ã©volutive et exploitable par des agents IA â€” sans dÃ©gradation de qualitÃ© dans le temps ?

---

## Verrous techniques identifiÃ©s

### Verrou 1 â€” Extraction de procÃ©dures Ã  partir de texte libre

Les tickets GLPI sont rÃ©digÃ©s en langage naturel non standardisÃ© par des techniciens et des utilisateurs finaux. L'information utile est noyÃ©e dans du bruit (formules de politesse, descriptions imprÃ©cises, fautes). Le LLM doit identifier un **pattern de rÃ©solution gÃ©nÃ©ralisable** Ã  partir de cas individuels, ce qui dÃ©passe la simple synthÃ¨se de texte.

**Incertitudes** :
- Comment distinguer une rÃ©solution rÃ©ellement transposable d'un cas trop spÃ©cifique ?
- Comment extraire des Ã©tapes de procÃ©dure quand le technicien n'a pas documentÃ© toutes ses actions ?
- Quel seuil de rÃ©currence (nombre de tickets similaires) est nÃ©cessaire pour justifier la crÃ©ation d'une procÃ©dure ?

### Verrou 2 â€” Maintien de la qualitÃ© d'une base RAG auto-alimentÃ©e

La base de connaissances s'enrichit en continu via l'Enrichisseur. Cela crÃ©e un risque de **data drift** : procÃ©dures obsolÃ¨tes, contradictions entre anciennes et nouvelles procÃ©dures, redondances qui polluent les rÃ©sultats de recherche sÃ©mantique.

**Incertitudes** :
- Comment dÃ©tecter qu'une procÃ©dure existante est rendue obsolÃ¨te par une nouvelle ?
- Comment mesurer la qualitÃ© globale du RAG au fil de son enrichissement ?
- Comment Ã©viter que l'accumulation de documents dÃ©grade la prÃ©cision de retrieval plutÃ´t que de l'amÃ©liorer ?

### Verrou 3 â€” Boucle de feedback et cercle vertueux

Le systÃ¨me repose sur un cercle : tickets rÃ©solus â†’ procÃ©dures gÃ©nÃ©rÃ©es â†’ RAG enrichi â†’ IA plus performante â†’ meilleure rÃ©solution â†’ plus de matiÃ¨re pour l'Enrichisseur. La convergence de cette boucle n'est pas garantie.

**Incertitudes** :
- La boucle converge-t-elle rÃ©ellement (amÃ©lioration mesurable) ou diverge-t-elle (accumulation de bruit) ?
- Comment mesurer l'impact de chaque procÃ©dure ajoutÃ©e sur la performance globale du systÃ¨me ?
- Quel est le ratio optimal entre procÃ©dures gÃ©nÃ©rÃ©es automatiquement et procÃ©dures validÃ©es humainement ?

---

## Ã‰valuation Frascati

| CritÃ¨re | Ã‰valuation | Justification |
|---------|------------|---------------|
| **NouveautÃ©** | âœ… Fort | L'Ã©tat de l'art ne propose pas de solution Ã©tablie pour la capitalisation automatique continue de connaissances IT opÃ©rationnelles via LLM avec boucle de feedback |
| **CrÃ©ativitÃ©** | âœ… Fort | La chaÃ®ne complÃ¨te ticket â†’ analyse LLM â†’ extraction pattern â†’ gÃ©nÃ©ration procÃ©dure â†’ validation humaine â†’ vectorisation â†’ rÃ©utilisation est une architecture originale |
| **Incertitude** | âœ… Fort | Trois verrous identifiÃ©s sans solution connue : extraction fiable, maintien qualitÃ© RAG, convergence de la boucle |
| **SystÃ©matisation** | âš ï¸ Ã€ documenter | NÃ©cessite des mÃ©triques de suivi, des protocoles d'expÃ©rimentation comparatifs, des critÃ¨res de validation formalisÃ©s |
| **TransfÃ©rabilitÃ©** | âœ… Moyen | La mÃ©thodologie est gÃ©nÃ©ralisable Ã  tout contexte de support IT, voire au-delÃ  (support client, maintenance industrielle) |

---

## Preuves Ã  constituer

- Logs de l'Enrichisseur : tickets analysÃ©s, procÃ©dures proposÃ©es, taux de validation/rejet
- MÃ©triques RAG : prÃ©cision de retrieval avant/aprÃ¨s enrichissement
- ExpÃ©rimentations comparatives : critÃ¨res de filtrage testÃ©s, seuils de rÃ©currence
- Exemples de procÃ©dures contradictoires ou obsolÃ¨tes dÃ©tectÃ©es (ou non)
- Ã‰volution du taux de rÃ©solution automatique corrÃ©lÃ©e Ã  l'enrichissement

---

---

# OPÃ‰RATION R&D 2 â€” Qualification et rÃ©solution autonome de tickets IT avec indice de confiance calibrÃ©

**Composants concernÃ©s** : Assistant Ticket + Sentinel (volet dÃ©cision IA)
**Potentiel CIR** : ğŸŸ¢ FORT (Assistant Ticket) + ğŸŸ¡ MOYEN (Sentinel)

---

## ProblÃ©matique scientifique

> Comment un systÃ¨me IA peut-il qualifier de maniÃ¨re fiable la difficultÃ© d'un ticket IT et rÃ©soudre les cas simples de faÃ§on autonome â€” tout en garantissant un niveau de confiance calibrÃ© et un fallback sÃ»r vers l'humain lorsque l'incertitude est trop Ã©levÃ©e ?

---

## Verrous techniques identifiÃ©s

### Verrou 1 â€” Qualification automatique en 4 niveaux de difficultÃ©

L'Assistant Ticket doit classer chaque ticket entrant en SIMPLE / MOYEN / COMPLEXE / EXPERT. Cette classification dÃ©termine directement le traitement : rÃ©solution IA autonome vs escalade humaine. Une erreur de classification a des consÃ©quences opÃ©rationnelles directes.

**Incertitudes** :
- Comment un LLM peut-il Ã©valuer la difficultÃ© rÃ©elle d'un ticket Ã  partir d'une description souvent incomplÃ¨te ou ambiguÃ« ?
- Comment gÃ©rer le cas critique du faux SIMPLE (ticket classÃ© simple mais qui nÃ©cessitait un expert) ?
- Comment intÃ©grer le contexte historique du client (tickets passÃ©s, infrastructure) dans la qualification ?

### Verrou 2 â€” Calibration de l'indice de confiance

Le systÃ¨me SAFEGUARD utilise un seuil de confiance (> 80% pour exÃ©cution automatique en L1). Ce seuil conditionne le degrÃ© d'autonomie du systÃ¨me. Or, les LLM sont notoirement **mal calibrÃ©s** en termes de confiance â€” ils expriment souvent une haute confiance mÃªme lorsqu'ils se trompent.

**Incertitudes** :
- Comment obtenir un score de confiance fiable d'un LLM pour des actions IT concrÃ¨tes ?
- Comment calibrer ce score (faire en sorte que "80% de confiance" corresponde rÃ©ellement Ã  80% de rÃ©ussite) ?
- Le seuil optimal est-il universel ou doit-il varier par type d'action, par client, par complexitÃ© ?

### Verrou 3 â€” RÃ©solution autonome avec actions irrÃ©versibles

RÃ©soudre un ticket simple implique des actions rÃ©elles : reset de mot de passe AD, dÃ©blocage de compte, crÃ©ation d'utilisateur. Ces actions ont des effets concrets et certaines sont irrÃ©versibles.

**Incertitudes** :
- Comment valider qu'une procÃ©dure RAG retrouvÃ©e est applicable au cas prÃ©cis du ticket courant ?
- Comment dÃ©tecter une mauvaise application de procÃ©dure avant l'exÃ©cution de l'action ?
- Quel protocole de rollback pour les actions partiellement exÃ©cutÃ©es en cas d'erreur dÃ©tectÃ©e en cours ?

### Verrou 4 â€” DÃ©cision autonome dans le monitoring proactif (Sentinel)

Sentinel doit prendre des dÃ©cisions en temps rÃ©el : identifier les clients impactÃ©s par une panne, dÃ©terminer s'il s'agit d'une panne FAI ou infrastructure, dÃ©cider quand et qui alerter.

**Incertitudes** :
- Comment l'IA peut-elle distinguer automatiquement une panne FAI d'une panne infrastructure WIDIP sans intervention humaine ? (Feature planifiÃ©e : "VÃ©rification FAI autonome")
- Comment le parsing de hostnames (format CLIENT-SITE-VILLE-DEVICE) peut-il gÃ©rer les cas non conformes au pattern standard ?
- Comment corrÃ©ler automatiquement un Ã©quipement Observium DOWN avec les bonnes entitÃ©s GLPI quand les nomenclatures ne sont pas parfaitement alignÃ©es ?

---

## Ã‰valuation Frascati

| CritÃ¨re | Ã‰valuation | Justification |
|---------|------------|---------------|
| **NouveautÃ©** | âœ… Fort | La calibration de confiance d'un LLM pour des actions IT opÃ©rationnelles avec consÃ©quences rÃ©elles est un problÃ¨me ouvert dans la littÃ©rature |
| **CrÃ©ativitÃ©** | âœ… Fort | Le couplage qualification â†’ rÃ©solution â†’ flagging "IA_NON_RESOLU" â†’ enrichissement ultÃ©rieur est une boucle de feedback innovante |
| **Incertitude** | âœ… Fort | Quatre verrous identifiÃ©s, dont la calibration de confiance qui est un problÃ¨me de recherche reconnu en IA |
| **SystÃ©matisation** | âš ï¸ Ã€ documenter | NÃ©cessite matrice de confusion qualification, logs de rÃ©solution, mesure taux d'erreur, protocole de test |
| **TransfÃ©rabilitÃ©** | âœ… Fort | La mÃ©thodologie de qualification par confiance calibrÃ©e est transposable Ã  tout domaine d'automatisation par IA |

---

## Preuves Ã  constituer

- Matrice de confusion : qualification prÃ©dite vs qualification rÃ©elle (sur un Ã©chantillon validÃ© humainement)
- Courbe de calibration : confiance exprimÃ©e vs taux de rÃ©ussite effectif
- Logs de rÃ©solution : tickets rÃ©solus automatiquement, incidents causÃ©s par l'IA
- Historique des tickets "IA_NON_RESOLU" : analyse post-rÃ©solution humaine
- Documentation des itÃ©rations sur les seuils de confiance
- Pour Sentinel : taux de faux positifs/nÃ©gatifs sur l'identification automatique des pannes

---

---

# OPÃ‰RATION R&D 3 â€” Gouvernance de sÃ©curitÃ© pour systÃ¨me multi-agents IA opÃ©rationnel

**Composants concernÃ©s** : SAFEGUARD (framework complet)
**Potentiel CIR** : ğŸŸ¢ FORT

---

## ProblÃ©matique scientifique

> Comment concevoir un framework de gouvernance garantissant la sÃ©curitÃ© d'un systÃ¨me multi-agents IA (3 workflows autonomes + 1 agent conversationnel) opÃ©rant sur des systÃ¨mes de production IT (Active Directory, ticketing, monitoring rÃ©seau) avec des actions pouvant Ãªtre irrÃ©versibles ?

---

## Verrous techniques identifiÃ©s

### Verrou 1 â€” ModÃ¨le de sÃ©curitÃ© multi-niveaux pour agents IA

Le framework SAFEGUARD dÃ©finit 6 niveaux de sensibilitÃ© (L0 Ã  L4_SMS) croisÃ©s avec 5 niveaux d'accrÃ©ditation utilisateur (N0 Ã  N4). Cette matrice dÃ©termine dynamiquement ce qu'un agent IA peut faire, ce qu'il doit demander, et ce qu'il ne peut jamais faire.

**Incertitudes** :
- Comment dÃ©finir la bonne granularitÃ© de niveaux ? Trop peu = risque de sÃ©curitÃ©, trop = friction opÃ©rationnelle
- Comment gÃ©rer les cas limites entre deux niveaux ?
- Le modÃ¨le est-il stable lorsque de nouveaux outils MCP sont ajoutÃ©s, ou nÃ©cessite-t-il une rÃ©Ã©valuation globale ?

### Verrou 2 â€” CohÃ©rence dans une architecture distribuÃ©e

Le double systÃ¨me SAFEGUARD (backend PostgreSQL + MCP Server Python) a gÃ©nÃ©rÃ© un problÃ¨me de recherche concret : aprÃ¨s approbation humaine cÃ´tÃ© backend, le MCP Server re-bloquait l'action. La solution (`_bypass_safeguard`) implique de maintenir la cohÃ©rence d'Ã©tat entre deux systÃ¨mes indÃ©pendants dans un contexte asynchrone.

**Incertitudes** :
- Comment garantir la synchronisation des niveaux de sÃ©curitÃ© entre deux systÃ¨mes indÃ©pendants ?
- Comment prÃ©venir les race conditions entre l'approbation et l'exÃ©cution dans un flux SSE asynchrone ?
- Comment auditer de maniÃ¨re fiable les actions exÃ©cutÃ©es via bypass dans un systÃ¨me distribuÃ© ?

### Verrou 3 â€” Orchestration sÃ©curisÃ©e de workflows autonomes chaÃ®nÃ©s

Sentinel peut dÃ©clencher l'Assistant Ticket, qui peut gÃ©nÃ©rer des actions nÃ©cessitant des validations SAFEGUARD. Cette chaÃ®ne crÃ©e des scÃ©narios complexes : un workflow autonome dÃ©clenche un autre workflow qui dÃ©clenche une action L3 nÃ©cessitant une validation humaine, pendant que le premier workflow continue de tourner.

**Incertitudes** :
- Comment gÃ©rer les dÃ©pendances entre actions dans une chaÃ®ne de workflows ?
- Comment Ã©viter les deadlocks quand un workflow attend une approbation pendant qu'un autre gÃ©nÃ¨re de nouvelles demandes ?
- Comment prioriser les demandes SAFEGUARD quand plusieurs workflows sollicitent simultanÃ©ment l'humain ?

### Verrou 4 â€” Confiance dynamique et niveau L4_SMS spÃ©cialisÃ©

Le niveau L4_SMS a Ã©tÃ© crÃ©Ã© spÃ©cifiquement pour l'alerte SMS client â€” un cas oÃ¹ ni le blocage total (L4) ni la simple validation (L3) ne suffisaient. Il intÃ¨gre une interface de sÃ©lection de contact avec recommandation IA et apprentissage par feedback humain (cache Redis mis Ã  jour si l'humain choisit un contact diffÃ©rent).

**Incertitudes** :
- Comment le systÃ¨me apprend-il quel contact recommander au fil du temps ?
- Le score d'activitÃ© DirEnt est-il un proxy fiable pour la pertinence du contact ?
- Comment gÃ©rer la dÃ©gradation de la recommandation quand les contacts changent de poste ?

---

## Ã‰valuation Frascati

| CritÃ¨re | Ã‰valuation | Justification |
|---------|------------|---------------|
| **NouveautÃ©** | âœ… Fort | TrÃ¨s peu de frameworks de contrÃ´le d'agents IA en production documentÃ©s dans la littÃ©rature, a fortiori pour du multi-agents sur systÃ¨mes IT critiques |
| **CrÃ©ativitÃ©** | âœ… Fort | L'architecture L0-L4_SMS avec confiance dynamique, approbation inline SSE, bypass post-approbation et apprentissage du contact recommandÃ© est une conception originale |
| **Incertitude** | âœ… Fort | Quatre verrous identifiÃ©s, dont le problÃ¨me de cohÃ©rence distribuÃ©e qui a Ã©tÃ© rencontrÃ© et rÃ©solu expÃ©rimentalement (double blocage) |
| **SystÃ©matisation** | âœ… Moyen | Le bug double-blocage et sa rÃ©solution constituent une trace d'expÃ©rimentation documentÃ©e ; l'audit trail fournit des donnÃ©es |
| **TransfÃ©rabilitÃ©** | âœ… Fort | Le framework est gÃ©nÃ©ralisable Ã  tout contexte d'agents IA opÃ©rant sur des systÃ¨mes avec actions Ã  risque (industrie, santÃ©, finance) |

---

## Preuves Ã  constituer

- Ã‰tat de l'art : frameworks de contrÃ´le d'agents IA existants (LangChain guardrails, Microsoft AutoGen safety, etc.) et leurs limites
- Historique des itÃ©rations sur le modÃ¨le de niveaux (ajouts, modifications, le cas L4_SMS)
- Documentation du bug double-blocage : analyse, hypothÃ¨ses testÃ©es, solution retenue
- Logs SAFEGUARD : demandes approuvÃ©es/rejetÃ©es, temps de rÃ©ponse, taux d'utilisation par niveau
- Analyse des near-misses : cas oÃ¹ SAFEGUARD a bloquÃ© une action potentiellement dangereuse
- Ã‰volution du cache rÃ©fÃ©rent : pertinence des recommandations au fil du temps

---

---

# SynthÃ¨se des 3 opÃ©rations R&D

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PLATEFORME IA WIDIP â€” 3 AXES R&D                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  OR1 â€” CAPITALISATION AUTO          OR2 â€” QUALIFICATION AUTONOME    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Enrichisseur ğŸŸ¢     â”‚            â”‚ Assistant Ticket ğŸŸ¢ â”‚         â”‚
â”‚  â”‚ RAG Ã©volutif  ğŸŸ¡    â”‚            â”‚ Sentinel (dÃ©c.) ğŸŸ¡  â”‚         â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚  â”‚ 3 verrous :         â”‚            â”‚ 4 verrous :         â”‚         â”‚
â”‚  â”‚ â€¢ Extraction texte  â”‚            â”‚ â€¢ Classification    â”‚         â”‚
â”‚  â”‚ â€¢ QualitÃ© RAG       â”‚            â”‚ â€¢ Calibration conf. â”‚         â”‚
â”‚  â”‚ â€¢ Convergence boucleâ”‚            â”‚ â€¢ Actions irrÃ©vers. â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ â€¢ DÃ©cision Sentinel â”‚         â”‚
â”‚                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                      â”‚
â”‚              OR3 â€” GOUVERNANCE SÃ‰CURITÃ‰                              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚ SAFEGUARD ğŸŸ¢                    â”‚                    â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”‚
â”‚              â”‚ 4 verrous :                     â”‚                    â”‚
â”‚              â”‚ â€¢ ModÃ¨le multi-niveaux          â”‚                    â”‚
â”‚              â”‚ â€¢ CohÃ©rence distribuÃ©e           â”‚                    â”‚
â”‚              â”‚ â€¢ Orchestration chaÃ®nÃ©e          â”‚                    â”‚
â”‚              â”‚ â€¢ Confiance dynamique + L4_SMS   â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TOTAL : 11 verrous techniques Â· 5 critÃ¨res Frascati Ã©valuÃ©s       â”‚
â”‚  Composants Ã©ligibles : 5 sur 9 analysÃ©s                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Ã‰valuation globale Frascati â€” Vue croisÃ©e

| CritÃ¨re | OR1 Capitalisation | OR2 Qualification | OR3 Gouvernance |
|---------|-------------------|-------------------|-----------------|
| **NouveautÃ©** | âœ… Fort | âœ… Fort | âœ… Fort |
| **CrÃ©ativitÃ©** | âœ… Fort | âœ… Fort | âœ… Fort |
| **Incertitude** | âœ… Fort | âœ… Fort | âœ… Fort |
| **SystÃ©matisation** | âš ï¸ Ã€ documenter | âš ï¸ Ã€ documenter | âœ… Moyen |
| **TransfÃ©rabilitÃ©** | âœ… Moyen | âœ… Fort | âœ… Fort |

---

# Verdict et conditions d'Ã©ligibilitÃ©

## Ce qui est fort

Les trois opÃ©rations R&D reposent sur des **problÃ¨mes de recherche lÃ©gitimes** :
- La capitalisation automatique par boucle d'apprentissage (OR1) est un sujet ouvert
- La calibration de confiance d'un LLM pour des actions IT consÃ©quentes (OR2) est un problÃ¨me reconnu
- La gouvernance de sÃ©curitÃ© multi-agents en production (OR3) est trÃ¨s peu documentÃ©e dans la littÃ©rature

## Ce qui conditionne l'Ã©ligibilitÃ©

Le **critÃ¨re de systÃ©matisation** est le point faible actuel. Pour chaque opÃ©ration, il faut constituer :
- Un Ã©tat de l'art formel (publications, solutions existantes, leurs limites)
- Des hypothÃ¨ses formalisÃ©es avant chaque expÃ©rimentation
- Des protocoles de test avec mÃ©triques mesurables
- Des rÃ©sultats chiffrÃ©s (positifs ET nÃ©gatifs)
- Un journal de bord R&D avec chronologie des itÃ©rations

## Risque principal en cas de contrÃ´le

Un expert MESR pourrait argumenter que le projet "assemble des briques existantes" (Mistral API + pgvector + FastAPI + Redis). La clÃ© de la dÃ©fense est de dÃ©montrer que **les problÃ¨mes de recherche rÃ©sident dans l'orchestration, la fiabilitÃ© et les boucles d'apprentissage** â€” pas dans les composants individuels.

## Recommandation

Le projet est **Ã©ligible sous condition** de produire la documentation R&D manquante. Les verrous techniques sont rÃ©els et les incertitudes documentables. La prioritÃ© est de transformer la documentation opÃ©rationnelle existante en documentation scientifique structurÃ©e.

---

# Prochaines Ã©tapes

1. **Constituer l'Ã©tat de l'art** pour chaque opÃ©ration (publications, solutions concurrentes)
2. **Formaliser les hypothÃ¨ses** testÃ©es ou Ã  tester pour chaque verrou
3. **Collecter les mÃ©triques** : logs, taux de rÃ©ussite, benchmarks comparatifs
4. **RÃ©diger le dossier technique** CIR avec le template MESR pour chaque opÃ©ration
5. **PrÃ©parer les preuves matÃ©rielles** : commits Git datÃ©s, logs d'expÃ©rimentation, comptes-rendus

---

*Analyse rÃ©alisÃ©e sur la base de CLAUDE_CONTEXT.md (v2.2.0) et ARCHITECTURE_FONCTIONNELLE_WIDIP.md (v2.0)*
*RÃ©fÃ©rentiel : Manuel de Frascati (OCDE), Guide CIR 2025 (MESR), Loi de Finances 2025*
